### For details see: https://github.com/openai/codex/blob/main/docs/config.md

##
## General configurations
##

# OS sandbox policy
# read-only | workspace-write | danger-full-access
sandbox_mode = "danger-full-access"

# When to prompt for approval.
# untrusted | on-failure | on-request | never
approval_policy = "never"

# Enable web search tool
tools.web_search = true

##
## MCP Configurations
##

# Streamable HTTP requires the experimental rmcp client
experimental_use_rmcp_client = true

# # Context7 - Up-to-date library and framework documentation
# [mcp_servers.context7]
# url = "https://mcp.context7.com/mcp"

# # Github Offical MCP (requires Personal Access Token or future OAUTH login via `codex mcp login github`)
# [mcp_servers.github]
# url = "https://api.githubcopilot.com/mcp/"
# bearer_token = "<token>"

##
## Project/folder permissions
##

# Trust the folder we will be working in during this lab
[projects."/home/project/eifkl-Expense-Tracking-Demo---Codex"]
trust_level = "trusted"

##
## OpenAI providers
##

[model_providers.sn-openai]
name = "OpenAI"
base_url = "https://api.openai.com/v1"
env_key = "OPENAI_API_KEY"
http_headers = { "SN-Event" = "techxchange2025" }
wire_api = "responses"

[profiles.gpt-5]
model = "gpt-5"
model_provider = "sn-openai"
model_reasoning_effort = "low"

[profiles.gpt-5-mini]
model = "gpt-5-mini"
model_provider = "sn-openai"

[profiles.gpt-5-nano]
model = "gpt-5-nano"
model_provider = "sn-openai"

[profiles.gpt-5-codex]
model = "gpt-5-codex"
model_provider = "sn-openai"

[profiles.gpt-5-codex-low]
model = "gpt-5-codex"
model_provider = "sn-openai"
model_reasoning_effort = "low"

[profiles.gpt-5-codex-medium]
model = "gpt-5-codex"
model_provider = "sn-openai"
model_reasoning_effort = "medium"

[profiles.gpt-5-codex-high]
model = "gpt-5-codex"
model_provider = "sn-openai"
model_reasoning_effort = "high"

##
## watsonx providers
##

[model_providers.watsonx]
name = "watsonx"
base_url = "http://0.0.0.0:4000"
http_headers = { "X-SN-Event" = "techxchange2025" }

[profiles.llama-4]
model = "watsonx/meta-llama/llama-4-maverick-17b-128e-instruct-fp8"
model_provider = "watsonx"

[profiles.granite-3]
model = "watsonx/ibm/granite-3-3-8b-instruct"
model_provider = "watsonx"

[profiles.granite-4]
model = "watsonx/ibm/granite-4-h-small"
model_provider = "watsonx"

[profiles.gpt-oss]
model = "watsonx/openai/gpt-oss-120b"
model_provider = "watsonx"
